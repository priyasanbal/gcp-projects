# ğŸ§¾ Spark-Based Big Data Processing on GCP (Dataproc + BigQuery)

## ğŸ“Œ Project Overview

This project demonstrates **large-scale data processing** using **Apache Spark on Google Cloud Dataproc**.  
Retail data is ingested from **Google Cloud Storage (GCS)**, processed using **PySpark**, and loaded into  
**Google BigQuery** for analytics.

The project is designed to reflect **real-world Data Engineering practices** on GCP.

## ğŸ§° Technologies Used

- **Apache Spark (PySpark)**
- **Google Cloud Dataproc**
- **Google Cloud Storage (GCS)**
- **Google BigQuery**
- **Python**
- **YAML (Configuration-driven design)**

---

## ğŸ“ Project Structure

The directory layout of the \texttt{spark-online-retail} project is shown below:

```
spark-online-retail/
â”‚
â”œâ”€â”€ spark_jobs/
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ orders_processing.py
â”‚   â”œâ”€â”€ customers_processing.py
â”‚   â””â”€â”€ country_sales_processing.py
â”‚
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.yaml
â”‚
â”œâ”€â”€ dataproc/
â”‚   â””â”€â”€ submit_job.sh
â”‚
â””â”€â”€ README.md
```

---

## ğŸ“‚ Input Data (GCS)

Raw retail data is stored in Google Cloud Storage:
gs://superstore-retail-code/raw-data/


Spark reads **all CSV files** present in this directory, enabling scalable and distributed ingestion.

---

## ğŸ”„ Data Processing Flow
### 1ï¸âƒ£ Orders Processing

Reads orders data from GCS

Parses dates and casts numeric fields

Writes cleaned data to BigQuery orders table

### 2ï¸âƒ£ Customers Processing

Extracts customer-related attributes

Removes duplicate records

Writes data to BigQuery customers table

### 3ï¸âƒ£ Country-Level Sales Aggregation

Groups data by country

Computes total sales, profit, and quantity

Writes aggregated results to BigQuery country_sales table

## ğŸš€ Running the Spark Job on Dataproc

Submit the Spark job using Cloud Shell or any environment with gcloud installed.

## ğŸª£ BigQuery Temporary GCS Bucket

The Sparkâ€“BigQuery connector requires a temporary GCS bucket for staging data.

superstore-bq-temp


This bucket is used internally during Spark â†’ BigQuery writes and is automatically cleaned up.

## ğŸ“Š BigQuery Output Tables

| **Table Name**   | **Description**                          |
|------------------|------------------------------------------|
| `orders`         | Cleaned order-level data                 |
| `customers`      | Customer master data                     |
| `country_sales`  | Aggregated country-level sales data      |

---

## ğŸ”‘ Key Spark Concepts Demonstrated

- **Transformations vs Actions**
- **Lazy Evaluation**
- **Distributed file reads from GCS**
- **Aggregations and GroupBy operations**
- **Sparkâ€“BigQuery Connector usage**
- **Cloud-native Spark execution on Dataproc**
